{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import FisherLDA, Perceptron, LogReg\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.3\n",
    "pm1_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"testing_accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "pm2_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"testing_accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "pm3_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"testing_accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "pm4_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"testing_accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "\n",
    "for _ in range(10):\n",
    "    # Read the csv file\n",
    "    dataset = pd.read_csv('../dataset.csv')\n",
    "    \n",
    "    # Convert cancer type into a new column\n",
    "    dataset[\"cancer_type\"] = pd.factorize(dataset[\"diagnosis\"])[0]\n",
    "    dataset[\"cancer_type\"] = dataset[\"cancer_type\"].replace(0, -1)\n",
    "    dataset.head()\n",
    "\n",
    "    # Remove ID and diagnosis from the dataset\n",
    "    dataset.drop(['id', 'diagnosis'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop NA rows because in first one we shouldn't use NA\n",
    "    dropped_dataset = dataset.dropna()\n",
    "\n",
    "    # Get pm2 ready\n",
    "    shuffled_dataset2 = dropped_dataset.sample(frac=1, random_state=2)\n",
    "    shuffled_dataset2.head()\n",
    "\n",
    "    # Get pm4 ready\n",
    "    dataset = pd.read_csv('../dataset.csv')\n",
    "    # Shuffling the dataset column wise\n",
    "    dataset_column_shuffled = dataset.sample(frac=1, axis=1)\n",
    "    dataset_column_shuffled.head()\n",
    "    # Convert cancer type into a new column\n",
    "    dataset_column_shuffled[\"cancer_type\"] = pd.factorize(dataset_column_shuffled[\"diagnosis\"])[0]\n",
    "    dataset_column_shuffled[\"cancer_type\"] = dataset_column_shuffled[\"cancer_type\"].replace(0, -1)\n",
    "    # Remove ID and diagnosis from the dataset\n",
    "    dataset_column_shuffled.drop(['id', 'diagnosis'], axis=1, inplace=True)\n",
    "    dataset_column_shuffled.head()\n",
    "\n",
    "    pm_1 = Perceptron(dataset=dropped_dataset)\n",
    "    print(\"Executing model : pm_1\")\n",
    "    w = pm_1.epoch_train(epoch=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    pm1_details[\"testing_accuracy_avg\"] += pm_1.max_accuracy[\"confusion_matrix\"][0] / 10\n",
    "    pm1_details[\"precision_avg\"] += pm_1.max_accuracy[\"confusion_matrix\"][1] / 10\n",
    "    pm1_details[\"f1_avg\"] += pm_1.max_accuracy[\"confusion_matrix\"][2] / 10\n",
    "    pm1_details[\"recall_avg\"] += pm_1.max_accuracy[\"confusion_matrix\"][3] / 10\n",
    "\n",
    "    pm_2 = Perceptron(dataset=shuffled_dataset2, is_normalized=False)\n",
    "    print(\"Executing model : pm_2\")\n",
    "    w = pm_2.epoch_train(epoch=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    pm2_details[\"testing_accuracy_avg\"] += pm_2.max_accuracy[\"confusion_matrix\"][0] / 10\n",
    "    pm2_details[\"precision_avg\"] += pm_2.max_accuracy[\"confusion_matrix\"][1] / 10\n",
    "    pm2_details[\"f1_avg\"] += pm_2.max_accuracy[\"confusion_matrix\"][2] / 10\n",
    "    pm2_details[\"recall_avg\"] += pm_2.max_accuracy[\"confusion_matrix\"][3] / 10\n",
    "\n",
    "    pm_3 = Perceptron(dataset=dropped_dataset, is_normalized=True)\n",
    "    print(\"Executing model : pm_3\")\n",
    "    w = pm_3.epoch_train(epoch=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    pm3_details[\"testing_accuracy_avg\"] += pm_3.max_accuracy[\"confusion_matrix\"][0] / 10\n",
    "    pm3_details[\"precision_avg\"] += pm_3.max_accuracy[\"confusion_matrix\"][1] / 10\n",
    "    pm3_details[\"f1_avg\"] += pm_3.max_accuracy[\"confusion_matrix\"][2] / 10\n",
    "    pm3_details[\"recall_avg\"] += pm_3.max_accuracy[\"confusion_matrix\"][3] / 10\n",
    "\n",
    "    pm_4 = Perceptron(dataset=dataset_column_shuffled, is_normalized=False)\n",
    "    print(\"Executing model : pm_4\")\n",
    "    w = pm_4.epoch_train(epoch=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    pm4_details[\"testing_accuracy_avg\"] += pm_4.max_accuracy[\"confusion_matrix\"][0] / 10\n",
    "    pm4_details[\"precision_avg\"] += pm_4.max_accuracy[\"confusion_matrix\"][1] / 10\n",
    "    pm4_details[\"f1_avg\"] += pm_4.max_accuracy[\"confusion_matrix\"][2] / 10\n",
    "    pm4_details[\"recall_avg\"] += pm_4.max_accuracy[\"confusion_matrix\"][3] / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm1_details={'training_accuracy': 0, 'testing_accuracy_avg': 0.8225806451612904, 'precision_avg': 0.9435483870967742, 'f1_avg': 0.8764044943820226, 'recall_avg': 0.818181818181818}\n",
      "pm2_details={'training_accuracy': 0, 'testing_accuracy_avg': 0.8763440860215054, 'precision_avg': 0.8759689922480619, 'f1_avg': 0.9076305220883532, 'recall_avg': 0.9416666666666664}\n",
      "pm3_details={'training_accuracy': 0, 'testing_accuracy_avg': 0.9731182795698924, 'precision_avg': 0.9999999999999999, 'f1_avg': 0.9822064056939503, 'recall_avg': 0.9650349650349652}\n",
      "pm4_details={'training_accuracy': 0, 'testing_accuracy_avg': 0.7978723404255318, 'precision_avg': 0.9421487603305787, 'f1_avg': 0.8571428571428573, 'recall_avg': 0.7862068965517243}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pm1_details=}\")\n",
    "print(f\"{pm2_details=}\")\n",
    "print(f\"{pm3_details=}\")\n",
    "print(f\"{pm4_details=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fldm1_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "fldm1_normalized_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "fldm2_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "fldm2_normalized_details = {\n",
    "    \"training_accuracy\": 0,\n",
    "    \"accuracy_avg\": 0,\n",
    "    \"precision_avg\": 0,\n",
    "    \"f1_avg\": 0,\n",
    "    \"recall_avg\": 0\n",
    "}\n",
    "\n",
    "for _ in range(10):\n",
    "    # Read the csv file\n",
    "    dataset = pd.read_csv('../dataset.csv')\n",
    "    dataset = dataset.sample(frac=1)\n",
    "\n",
    "    # Shuffling the dataset column wise\n",
    "    dataset_column_shuffled = dataset.sample(frac=1, axis=1)\n",
    "    # Convert cancer type into a new column\n",
    "    dataset_column_shuffled[\"cancer_type\"] = pd.factorize(\n",
    "        dataset_column_shuffled[\"diagnosis\"])[0]\n",
    "    # Malignant is 0 and Benign is 1\n",
    "    # Remove ID and diagnosis from the dataset\n",
    "    dataset_column_shuffled.drop(['id', 'diagnosis'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop NA rows because in first one we shouldn't use NA\n",
    "    dropped_dataset_column_shuffled = dataset_column_shuffled.dropna()\n",
    "\n",
    "    # Convert cancer type into a new column\n",
    "    dataset[\"cancer_type\"] = pd.factorize(dataset[\"diagnosis\"])[0]\n",
    "    # Malignant is 0 and Benign is 1\n",
    "    # Remove ID and diagnosis from the dataset\n",
    "    dataset.drop(['id', 'diagnosis'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop NA rows because in first one we shouldn't use NA\n",
    "    dropped_dataset = dataset.dropna()\n",
    "\n",
    "    # FLDM1\n",
    "    fldm1 = FisherLDA(dataset=dropped_dataset)\n",
    "    fldm1.train()\n",
    "    fldm1.check_testing_accuracy()\n",
    "    fldm1_details[\"training_accuracy\"] += fldm1.check_training_accuracy() / 10\n",
    "    fldm1_details[\"accuracy_avg\"] += fldm1.show_confusion_matrix()[0] / 10\n",
    "    fldm1_details[\"precision_avg\"] += fldm1.show_confusion_matrix()[1] / 10\n",
    "    fldm1_details[\"f1_avg\"] += fldm1.show_confusion_matrix()[2] / 10\n",
    "    fldm1_details[\"recall_avg\"] += fldm1.show_confusion_matrix()[3] / 10\n",
    "\n",
    "    # FLDM1 with Normalized Dataset\n",
    "    fldm1_normalized = FisherLDA(dataset=dataset, is_feature_engineered=1)\n",
    "    fldm1_normalized.train()\n",
    "    fldm1_normalized.check_testing_accuracy()\n",
    "    fldm1_normalized_details[\"training_accuracy\"] += fldm1_normalized.check_training_accuracy() / 10\n",
    "    fldm1_normalized_details[\"accuracy_avg\"] += fldm1_normalized.show_confusion_matrix()[\n",
    "        0] / 10\n",
    "    fldm1_normalized_details[\"precision_avg\"] += fldm1_normalized.show_confusion_matrix()[\n",
    "        1] / 10\n",
    "    fldm1_normalized_details[\"f1_avg\"] += fldm1_normalized.show_confusion_matrix()[\n",
    "        2] / 10\n",
    "    fldm1_normalized_details[\"recall_avg\"] += fldm1_normalized.show_confusion_matrix()[\n",
    "        3] / 10\n",
    "\n",
    "    # FLDM2\n",
    "    fldm2 = FisherLDA(dataset=dropped_dataset_column_shuffled)\n",
    "    fldm2.train()\n",
    "    fldm2.check_testing_accuracy()\n",
    "    fldm2_details[\"training_accuracy\"] += fldm2.check_training_accuracy() / 10\n",
    "    fldm2_details[\"accuracy_avg\"] += fldm2.show_confusion_matrix()[0] / 10\n",
    "    fldm2_details[\"precision_avg\"] += fldm2.show_confusion_matrix()[1] / 10\n",
    "    fldm2_details[\"f1_avg\"] += fldm2.show_confusion_matrix()[2] / 10\n",
    "    fldm2_details[\"recall_avg\"] += fldm2.show_confusion_matrix()[3] / 10\n",
    "\n",
    "    # FLDM2 with Normalized Dataset\n",
    "    fldm2_normalized = FisherLDA(\n",
    "        dataset=dataset_column_shuffled, is_feature_engineered=1)\n",
    "    fldm2_normalized.train()\n",
    "    fldm2_normalized.check_testing_accuracy()\n",
    "    fldm2_normalized_details[\"training_accuracy\"] += fldm2_normalized.check_training_accuracy() / 10\n",
    "    fldm2_normalized_details[\"accuracy_avg\"] += fldm2_normalized.show_confusion_matrix()[\n",
    "        0] / 10\n",
    "    fldm2_normalized_details[\"precision_avg\"] += fldm2_normalized.show_confusion_matrix()[\n",
    "        1] / 10\n",
    "    fldm2_normalized_details[\"f1_avg\"] += fldm2_normalized.show_confusion_matrix()[\n",
    "        2] / 10\n",
    "    fldm2_normalized_details[\"recall_avg\"] += fldm2_normalized.show_confusion_matrix()[\n",
    "        3] / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fldm1_details={'training_accuracy': 0.9777188328912467, 'accuracy_avg': 0.9596774193548389, 'precision_avg': 0.9746214371245424, 'f1_avg': 0.9492712616650933, 'recall_avg': 0.9270876800737039}\n",
      "fldm1_normalized_details={'training_accuracy': 0.978215223097113, 'accuracy_avg': 0.9617021276595744, 'precision_avg': 0.9757386548050596, 'f1_avg': 0.9514392175639453, 'recall_avg': 0.9300511851106507}\n",
      "fldm2_details={'training_accuracy': 0.9777188328912467, 'accuracy_avg': 0.9596774193548389, 'precision_avg': 0.9746214371245424, 'f1_avg': 0.9492712616650933, 'recall_avg': 0.9270876800737039}\n",
      "fldm2_normalized_details={'training_accuracy': 0.978215223097113, 'accuracy_avg': 0.9617021276595744, 'precision_avg': 0.9757386548050596, 'f1_avg': 0.9514392175639453, 'recall_avg': 0.9300511851106507}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{fldm1_details=}\")\n",
    "print(f\"{fldm1_normalized_details=}\")\n",
    "print(f\"{fldm2_details=}\")\n",
    "print(f\"{fldm2_normalized_details=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "final_permutations = np.zeros((45, 5))\n",
    "descent = (\"Batch\", \"Mini-Batch\", \"Stochastic\")\n",
    "threshold = (0.5, 0.3, 0.4, 0.6, 0.7)\n",
    "learning_rate = (0.01, 0.001, 0.0001)\n",
    "\n",
    "for _ in range(10):\n",
    "    dataset = pd.read_csv('../dataset.csv')\n",
    "    dataset = dataset.sample(frac=1)\n",
    "\n",
    "    # Convert cancer type into a new column\n",
    "    dataset[\"cancer_type\"] = pd.factorize(dataset[\"diagnosis\"])[0]\n",
    "    # Malignant is 0 and Benign is 1\n",
    "\n",
    "    # Remove ID and diagnosis from the dataset\n",
    "    dataset.drop(['id', 'diagnosis'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop NA rows because in first one we shouldn't use NA\n",
    "    dropped_dataset = dataset.dropna()\n",
    "    \n",
    "    # Logistic Regression 1\n",
    "    lr1 = LogReg(dataset=dropped_dataset)\n",
    "    matrix_of_45x5 = lr1.run_all_permutations(descents = descent, thresholds = threshold, learning_rates = learning_rate, epochs = 50, showGraph=False)\n",
    "    final_permutations += matrix_of_45x5 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54376658 0.54462366        nan        nan 0.53164326]\n",
      " [0.54880637 0.54677419 0.74254332 0.44122164 0.5678271 ]\n",
      " [0.80981432 0.79623656 0.82458753 0.77540712 0.80186759]\n",
      " [0.88567639 0.87741935 0.85509975 0.87015714 0.89391496]\n",
      " [0.87665782 0.87096774 0.85224137 0.86313099 0.89063055]\n",
      " [0.89575597 0.89139785 0.8721688  0.88200273 0.89315608]\n",
      " [0.89549072 0.89139785 0.87229142 0.88217296 0.89349931]\n",
      " [0.89787798 0.89139785 0.87229142 0.88217296 0.89349931]\n",
      " [0.89734748 0.89193548 0.87244919 0.88285543 0.89478136]\n",
      " [0.89734748 0.89193548 0.87334931 0.88273197 0.89349931]\n",
      " [0.89655172 0.89193548 0.87334931 0.88273197 0.89349931]\n",
      " [0.89628647 0.89193548 0.87334931 0.88273197 0.89349931]\n",
      " [0.89602122 0.89193548 0.87334931 0.88273197 0.89349931]\n",
      " [0.89681698 0.89193548 0.87334931 0.88273197 0.89349931]\n",
      " [0.89681698 0.89193548 0.87334931 0.88273197 0.89349931]\n",
      " [0.53183024 0.54677419        nan        nan 0.58256769]\n",
      " [0.59363395 0.58548387        nan        nan 0.49105971]\n",
      " [0.75862069 0.74569892        nan        nan 0.64182193]\n",
      " [0.85623342 0.84408602 0.83520651 0.84059711 0.874321  ]\n",
      " [0.84244032 0.83333333 0.82431622 0.836574   0.88519953]\n",
      " [0.90291777 0.90053763 0.88586106 0.89291315 0.90115267]\n",
      " [0.90344828 0.89946237 0.88302834 0.89147466 0.90115267]\n",
      " [0.90318302 0.90053763 0.88586106 0.89291315 0.90115267]\n",
      " [0.90371353 0.9        0.88407772 0.89202994 0.90115267]\n",
      " [0.90397878 0.90107527 0.88691045 0.89346843 0.90115267]\n",
      " [0.90450928 0.9016129  0.88802156 0.89404164 0.90115267]\n",
      " [0.90424403 0.9016129  0.88802156 0.89404164 0.90115267]\n",
      " [0.90503979 0.90215054 0.88813578 0.89458737 0.90214276]\n",
      " [0.90450928 0.90215054 0.88909751 0.89460409 0.90115267]\n",
      " [0.90477454 0.90215054 0.88909751 0.89460409 0.90115267]\n",
      " [0.83819629 0.82956989 0.86059904 0.82036438 0.82681899]\n",
      " [0.84562334 0.83763441 0.83152434 0.83757389 0.88560067]\n",
      " [0.86233422 0.8516129  0.84939365 0.84695109 0.87694739]\n",
      " [0.87082228 0.85430108 0.86112964 0.85103543 0.87333717]\n",
      " [0.88381963 0.86827957 0.86450126 0.86442314 0.88791391]\n",
      " [0.91140584 0.90591398 0.89444443 0.89973399 0.90724649]\n",
      " [0.90928382 0.90430108 0.89213008 0.89769339 0.90569712]\n",
      " [0.91087533 0.90645161 0.89788611 0.90007348 0.90405564]\n",
      " [0.9127321  0.90591398 0.90183779 0.89845608 0.89694275]\n",
      " [0.91352785 0.90860215 0.90432698 0.90148551 0.9005661 ]\n",
      " [0.91909814 0.91344086 0.90819672 0.90706287 0.90643474]\n",
      " [0.91909814 0.91344086 0.90810567 0.90705072 0.90643474]\n",
      " [0.91909814 0.91344086 0.90857814 0.90725272 0.90643474]\n",
      " [0.91856764 0.91344086 0.90772426 0.90686086 0.90643474]\n",
      " [0.9193634  0.91344086 0.90857814 0.90725272 0.90643474]]\n"
     ]
    }
   ],
   "source": [
    "# Order: Training Accuracy, Accuracy, Precision, recall, f1\n",
    "print(final_permutations)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "\n",
    "descent = (\"Batch\", \"Mini-Batch\", \"Stochastic\")\n",
    "threshold = (0.5, 0.3, 0.4, 0.6, 0.7)\n",
    "learning_rate = (0.01, 0.001, 0.0001)\n",
    "i = 0\n",
    "for d in descent:\n",
    "            for lr in learning_rate:\n",
    "                for t in threshold: \n",
    "                    g = [d, lr, t]\n",
    "                    for m in final_permutations[i]:\n",
    "                        g.append(m)\n",
    "                    result.append(g)\n",
    "                    i += 1\n",
    "df = pd.DataFrame(result,\n",
    "               columns =['Batch', 'Learning Rate', 'Threshold', 'Training Accuracy', 'Accuracy', 'Precision', 'F1', 'Recall'])\n",
    "df.to_csv(\"../Result/log_reg_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR With Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "final_permutations_normalised = np.zeros((45, 5))\n",
    "descent = (\"Batch\", \"Mini-Batch\", \"Stochastic\")\n",
    "threshold = (0.5, 0.3, 0.4, 0.6, 0.7)\n",
    "learning_rate = (0.01, 0.001, 0.0001)\n",
    "\n",
    "for _ in range(10):\n",
    "    dataset = pd.read_csv('../dataset.csv')\n",
    "    dataset = dataset.sample(frac=1)\n",
    "\n",
    "    # Convert cancer type into a new column\n",
    "    dataset[\"cancer_type\"] = pd.factorize(dataset[\"diagnosis\"])[0]\n",
    "    # Malignant is 0 and Benign is 1\n",
    "\n",
    "    # Remove ID and diagnosis from the dataset\n",
    "    dataset.drop(['id', 'diagnosis'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop NA rows because in first one we shouldn't use NA\n",
    "    dropped_dataset = dataset.dropna()\n",
    "\n",
    "    # Logistic Regression 1\n",
    "    lr2 = LogReg(dataset=dataset, is_feature_engineered=1)\n",
    "    matrix_of_45x5_normalised = lr2.run_all_permutations(\n",
    "        descents=descent, thresholds=threshold, learning_rates=learning_rate, epochs=50, showGraph=False)\n",
    "    final_permutations_normalised += matrix_of_45x5_normalised / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98608924 0.97659574 0.97784676 0.97545594 0.97330866]\n",
      " [0.98372703 0.97446809 0.963021   0.97255979 0.98243733]\n",
      " [0.98792651 0.97712766 0.97147909 0.97482025 0.97858269]\n",
      " [0.98661417 0.97287234 0.98552443 0.97047715 0.95655056]\n",
      " [0.98372703 0.96914894 0.98671509 0.96571317 0.94613126]\n",
      " [0.98897638 0.975      0.97593613 0.97142012 0.96744813]\n",
      " [0.98582677 0.97234043 0.9576864  0.97017843 0.98338917]\n",
      " [0.98871391 0.97393617 0.96667854 0.9716211  0.97695023]\n",
      " [0.98661417 0.9712766  0.98207414 0.96880882 0.95650744]\n",
      " [0.9839895  0.96968085 0.98674533 0.96659552 0.94774417]\n",
      " [0.98950131 0.97393617 0.97435362 0.9702613  0.96657857]\n",
      " [0.9855643  0.97287234 0.95929842 0.97096479 0.98338917]\n",
      " [0.98871391 0.97287234 0.96584144 0.970433   0.97543085]\n",
      " [0.98661417 0.97074468 0.98040747 0.96807141 0.95650744]\n",
      " [0.9839895  0.96968085 0.98674533 0.96659552 0.94774417]\n",
      " [0.98713911 0.97659574 0.97710971 0.97542836 0.97404544]\n",
      " [0.98372703 0.97606383 0.96292711 0.97447144 0.98649461]\n",
      " [0.98713911 0.97393617 0.9681975  0.97192216 0.97601712]\n",
      " [0.98740157 0.97180851 0.98128565 0.96895975 0.95740514]\n",
      " [0.9839895  0.96755319 0.98498358 0.96445784 0.94542518]\n",
      " [0.98897638 0.97340426 0.97431234 0.97128823 0.96880716]\n",
      " [0.98530184 0.9712766  0.95904414 0.96958891 0.98082359]\n",
      " [0.98845144 0.97180851 0.96576129 0.96954212 0.97376168]\n",
      " [0.98740157 0.9712766  0.98042398 0.96853456 0.95740514]\n",
      " [0.9839895  0.96755319 0.98498358 0.96445784 0.94542518]\n",
      " [0.98897638 0.97234043 0.97267299 0.96972573 0.96731462]\n",
      " [0.98530184 0.9712766  0.95904414 0.96958891 0.98082359]\n",
      " [0.98845144 0.9712766  0.96499938 0.96914714 0.97376168]\n",
      " [0.98740157 0.9712766  0.98042398 0.96853456 0.95740514]\n",
      " [0.9839895  0.96755319 0.98498358 0.96445784 0.94542518]\n",
      " [0.98188976 0.98031915 0.98045485 0.9797569  0.97911427]\n",
      " [0.97585302 0.97606383 0.96056764 0.97500764 0.98998392]\n",
      " [0.98372703 0.98138298 0.97497207 0.98005429 0.98522151]\n",
      " [0.98372703 0.97234043 0.98370614 0.96981028 0.95661966]\n",
      " [0.97952756 0.96702128 0.99169266 0.96450521 0.93947195]\n",
      " [0.98713911 0.97978723 0.98034686 0.97835692 0.97661102]\n",
      " [0.98005249 0.97553191 0.96111579 0.97461794 0.98861011]\n",
      " [0.98582677 0.98031915 0.97341239 0.97850547 0.98372898]\n",
      " [0.98530184 0.97287234 0.98372139 0.97025455 0.95748922]\n",
      " [0.98031496 0.96808511 0.99172513 0.96542151 0.94121108]\n",
      " [0.98713911 0.97978723 0.98034686 0.97835692 0.97661102]\n",
      " [0.98005249 0.975      0.96025034 0.97384702 0.98798714]\n",
      " [0.98582677 0.98031915 0.97341239 0.97850547 0.98372898]\n",
      " [0.9855643  0.97287234 0.98372139 0.97025455 0.95748922]\n",
      " [0.98057743 0.96808511 0.99172513 0.96542151 0.94121108]]\n"
     ]
    }
   ],
   "source": [
    "print(final_permutations_normalised)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "\n",
    "descent = (\"Batch\", \"Mini-Batch\", \"Stochastic\")\n",
    "threshold = (0.5, 0.3, 0.4, 0.6, 0.7)\n",
    "learning_rate = (0.01, 0.001, 0.0001)\n",
    "i = 0\n",
    "for d in descent:\n",
    "            for lr in learning_rate:\n",
    "                for t in threshold: \n",
    "                    g = [d, lr, t]\n",
    "                    for m in final_permutations_normalised[i]:\n",
    "                        g.append(m)\n",
    "                    result.append(g)\n",
    "                    i += 1\n",
    "df = pd.DataFrame(result,\n",
    "               columns =['Batch', 'Learning Rate', 'Threshold', 'Training Accuracy', 'Accuracy', 'Precision', 'F1', 'Recall'])\n",
    "df.to_csv(\"../Result/log_reg_norm_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
