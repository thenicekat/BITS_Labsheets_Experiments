{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B\n",
    "In general, a basic neural network architecture can be considered that consists of an input  \n",
    "layer, one or more hidden layers, and an output layer.  \n",
    "\n",
    "You  are  supposed  to  build  15  distinct  artificial  neural  network  classifiers  by  varying  one  or \n",
    "more paramours from the following list: \n",
    "\n",
    "- (i). Number of hidden layers â€“ 2 or 3\n",
    "- (ii) Total number of neurons in the hidden layer is 100 or 150\n",
    "- (iii) Activation function is from any of the following functions: tanh, sigmoid, ReLu\n",
    "\n",
    "---\n",
    "\n",
    "You need to train your network on the MNIST dataset. You can use any optimization algorithm \n",
    "like  stochastic  gradient  descent  or  Adam  optimizer.  You  need  to  evaluate  your  network's \n",
    "performance on a test set of images from the MNIST dataset. You can calculate the accuracy and \n",
    "confusion matrix to measure your network's performance. \n",
    "Perform a comparative study of these 15 models and figure out the best classifier. Do you have \n",
    "a classifier that  is not statistically significant from the best classifier? Detail the results with all \n",
    "explanations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Importing TorchVision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cpu\n",
      "torchvision version: 0.15.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAGkCAYAAACckEpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbhklEQVR4nO3df3DU953f8deaH2vgVntVsbSrICs6H5w9FiUNEECHQdCgQx0zxnJSbHcykCaMbQQ3VLi+YDpFl8khH1MYcpFNLlwOwwQOJjcYaKHGSkHCFHAxh2NKfEQ+RJDPklVksytkvCDx6R8qay/C4O96V2/t6vmY+U7Y7/f71vfNJ1/75Y/2u5/1OeecAAAwdJd1AwAAEEYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAcxkVRi+99JKKi4t19913a+LEiXr99detW+pXNTU18vl8CVsoFLJuq18cPnxY8+bNU0FBgXw+n3bv3p1w3DmnmpoaFRQUaMSIESorK9OZM2dsmk2jO43DokWL+twjU6dOtWk2jWprazV58mQFAgHl5eVp/vz5Onv2bMI5g+Ge+CLjkCn3RMaE0c6dO7V8+XKtWrVKp06d0kMPPaSKigpduHDBurV+9eCDD6q1tTW+nT592rqlftHV1aUJEyaorq7ulsfXrl2r9evXq66uTidOnFAoFNKcOXPU2dnZz52m153GQZLmzp2bcI/s37+/HzvsH42NjaqqqtLx48dVX1+v7u5ulZeXq6urK37OYLgnvsg4SBlyT7gM8Y1vfMM9/fTTCfvuv/9+94Mf/MCoo/63evVqN2HCBOs2zElyr7zySvz19evXXSgUci+88EJ83yeffOKCwaD76U9/atBh/7h5HJxzbuHChe6RRx4x6cdSe3u7k+QaGxudc4P3nrh5HJzLnHsiI2ZGV69e1cmTJ1VeXp6wv7y8XEePHjXqykZTU5MKCgpUXFysxx9/XOfOnbNuyVxzc7Pa2toS7g+/36+ZM2cOuvtDkhoaGpSXl6dx48Zp8eLFam9vt24p7SKRiCQpNzdX0uC9J24ehxsy4Z7IiDC6ePGienp6lJ+fn7A/Pz9fbW1tRl31vylTpmjr1q06cOCANm3apLa2NpWWlqqjo8O6NVM37oHBfn9IUkVFhbZt26aDBw9q3bp1OnHihGbPnq1YLGbdWto451RdXa3p06erpKRE0uC8J241DlLm3BNDrRvwwufzJbx2zvXZl80qKirifx4/frymTZum++67T1u2bFF1dbVhZwPDYL8/JGnBggXxP5eUlGjSpEkqKirSvn37VFlZadhZ+ixdulRvv/22jhw50ufYYLonPm8cMuWeyIiZ0ejRozVkyJA+/0XT3t7e5798BpNRo0Zp/Pjxampqsm7F1I0nCrk/+gqHwyoqKsrae2TZsmXau3evDh06pDFjxsT3D7Z74vPG4VYG6j2REWE0fPhwTZw4UfX19Qn76+vrVVpaatSVvVgspnfeeUfhcNi6FVPFxcUKhUIJ98fVq1fV2Ng4qO8PSero6FBLS0vW3SPOOS1dulS7du3SwYMHVVxcnHB8sNwTdxqHWxmw94ThwxOe7Nixww0bNsz9/Oc/d7/5zW/c8uXL3ahRo9z58+etW+s3K1ascA0NDe7cuXPu+PHj7uGHH3aBQGBQjEFnZ6c7deqUO3XqlJPk1q9f706dOuV+97vfOeece+GFF1wwGHS7du1yp0+fdk888YQLh8MuGo0ad55atxuHzs5Ot2LFCnf06FHX3NzsDh065KZNm+a+8pWvZN04PPPMMy4YDLqGhgbX2toa3z7++OP4OYPhnrjTOGTSPZExYeSccy+++KIrKipyw4cPd1//+tcTHl8cDBYsWODC4bAbNmyYKygocJWVle7MmTPWbfWLQ4cOOUl9toULFzrneh/lXb16tQuFQs7v97sZM2a406dP2zadBrcbh48//tiVl5e7e+65xw0bNszde++9buHChe7ChQvWbafcrcZAktu8eXP8nMFwT9xpHDLpnvA551z/zcMAAOgrI94zAgBkN8IIAGCOMAIAmCOMAADmCCMAgDnCCABgLqPCKBaLqaamZsAt8GeBsejFOPRiHD7FWPTKtHHIqM8ZRaNRBYNBRSIR5eTkWLdjirHoxTj0Yhw+xVj0yrRxyKiZEQAgOxFGAABzA+77jK5fv673339fgUCgz/eORKPRhP8dzBiLXoxDL8bhU4xFr4EwDs45dXZ2qqCgQHfddfu5z4B7z+i9995TYWGhdRsAgBRpaWm54/csDbiZUSAQkCRN17/VUA0z7gYAkKxuXdMR7Y//e/12BlwY3fjV3FAN01AfYQQAGev//97ti3zVe9oeYHjppZdUXFysu+++WxMnTtTrr7+erksBADJcWsJo586dWr58uVatWqVTp07poYceUkVFhS5cuJCOywEAMlxawmj9+vX63ve+p+9///t64IEHtGHDBhUWFmrjxo3puBwAIMOlPIyuXr2qkydPqry8PGF/eXm5jh492uf8WCymaDSasAEABpeUh9HFixfV09Oj/Pz8hP35+flqa2vrc35tba2CwWB847FuABh80vYAw81PTzjnbvlExcqVKxWJROJbS0tLuloCAAxQKX+0e/To0RoyZEifWVB7e3uf2ZIk+f1++f3+VLcBAMggKZ8ZDR8+XBMnTlR9fX3C/vr6epWWlqb6cgCALJCWD71WV1frO9/5jiZNmqRp06bpZz/7mS5cuKCnn346HZcDAGS4tITRggUL1NHRoR/+8IdqbW1VSUmJ9u/fr6KionRcDgCQ4QbcQqk3vhCqTI+wHBAAZLBud00N2vOFvuCP7zMCAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYG6odQPAQOIbmtw/EkPuGZ3iTlLr7LNf9VzTM/K655qi+9o914xc4vNcI0lt64d7rvmHSTs911zs6fJcI0lTfrnCc80fVh9P6lrZgJkRAMAcYQQAMJfyMKqpqZHP50vYQqFQqi8DAMgiaXnP6MEHH9SvfvWr+OshQ4ak4zIAgCyRljAaOnQosyEAwBeWlveMmpqaVFBQoOLiYj3++OM6d+7c554bi8UUjUYTNgDA4JLyMJoyZYq2bt2qAwcOaNOmTWpra1Npaak6OjpueX5tba2CwWB8KywsTHVLAIABLuVhVFFRoccee0zjx4/XN7/5Te3bt0+StGXLlluev3LlSkUikfjW0tKS6pYAAANc2j/0OmrUKI0fP15NTU23PO73++X3+9PdBgBgAEv754xisZjeeecdhcPhdF8KAJChUh5Gzz77rBobG9Xc3Kw33nhD3/rWtxSNRrVw4cJUXwoAkCVS/mu69957T0888YQuXryoe+65R1OnTtXx48dVVFSU6ksBALJEysNox44dqf6RAIAsx6rdSNqQB8YmVef8wzzXvD/z9z3XXJnqfbXl3GByKzS/PsH7atDZ6H98HPBc85d1c5O61hvjt3uuab52xXPNCx/M8VwjSQWvu6TqBisWSgUAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOhVIhSeop+7rnmvUvv5jUtcYNG55UHfrXNdfjuea//GSR55qhXcktKDrtl0s91wT+udtzjf+i98VVJWnkm28kVTdYMTMCAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjoVSIUnyn33fc83JTwqTuta4YR8kVZdtVrRO9Vxz7vLopK718n1/77kmct37Aqb5f3XUc81Al9wyrvCKmREAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwByrdkOS1N3a5rnmJ3/57aSu9RdzuzzXDHn79zzX/HrJTzzXJOtHF/+V55p3vznSc03PpVbPNZL05LQlnmvO/6n36xTr196LADEzAgAMAIQRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMyxUCqSlrv5WFJ19/y3f+m5pqfjQ881D5b8B881Z2b8recaSdr7s5mea/IuHU3qWsnwHfO+gGlxcv/3AklhZgQAMEcYAQDMeQ6jw4cPa968eSooKJDP59Pu3bsTjjvnVFNTo4KCAo0YMUJlZWU6c+ZMqvoFAGQhz2HU1dWlCRMmqK6u7pbH165dq/Xr16uurk4nTpxQKBTSnDlz1NnZ+aWbBQBkJ88PMFRUVKiiouKWx5xz2rBhg1atWqXKykpJ0pYtW5Sfn6/t27frqaee+nLdAgCyUkrfM2publZbW5vKy8vj+/x+v2bOnKmjR2/95FAsFlM0Gk3YAACDS0rDqK2tTZKUn5+fsD8/Pz9+7Ga1tbUKBoPxrbCwMJUtAQAyQFqepvP5fAmvnXN99t2wcuVKRSKR+NbS0pKOlgAAA1hKP/QaCoUk9c6QwuFwfH97e3uf2dINfr9ffr8/lW0AADJMSmdGxcXFCoVCqq+vj++7evWqGhsbVVpamspLAQCyiOeZ0eXLl/Xuu+/GXzc3N+utt95Sbm6u7r33Xi1fvlxr1qzR2LFjNXbsWK1Zs0YjR47Uk08+mdLGAQDZw3MYvfnmm5o1a1b8dXV1tSRp4cKFevnll/Xcc8/pypUrWrJkiT766CNNmTJFr732mgKBQOq6BgBkFZ9zzlk38VnRaFTBYFBlekRDfcOs20EG++1fT/Ze8/BPk7rWd3/3bzzX/N/pSXwQ/HqP9xrASLe7pgbtUSQSUU5Ozm3PZW06AIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5lL65XrAQPLAn/3Wc813x3tf8FSSNhf9T881M79d5bkmsPO45xogEzAzAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYY9VuZK2eSxHPNR3PPJDUtS7sveK55gc/2uq5ZuW/e9RzjSS5U0HPNYV/cSyJCznvNYCYGQEABgDCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmWCgV+Izrv34nqbrH//w/ea7Ztvq/eq55a6r3xVUlSVO9lzw4aqnnmrGbWj3XdJ8777kG2YeZEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHM+55yzbuKzotGogsGgyvSIhvqGWbcDpI374695rsl54b2krvV3f3AgqTqv7j/0fc81f/TnkaSu1dN0Lqk69J9ud00N2qNIJKKcnJzbnsvMCABgjjACAJjzHEaHDx/WvHnzVFBQIJ/Pp927dyccX7RokXw+X8I2dWoSX6YCABg0PIdRV1eXJkyYoLq6us89Z+7cuWptbY1v+/fv/1JNAgCym+dveq2oqFBFRcVtz/H7/QqFQkk3BQAYXNLynlFDQ4Py8vI0btw4LV68WO3t7Z97biwWUzQaTdgAAINLysOooqJC27Zt08GDB7Vu3TqdOHFCs2fPViwWu+X5tbW1CgaD8a2wsDDVLQEABjjPv6a7kwULFsT/XFJSokmTJqmoqEj79u1TZWVln/NXrlyp6urq+OtoNEogAcAgk/Iwulk4HFZRUZGamppuedzv98vv96e7DQDAAJb2zxl1dHSopaVF4XA43ZcCAGQozzOjy5cv6913342/bm5u1ltvvaXc3Fzl5uaqpqZGjz32mMLhsM6fP6/nn39eo0eP1qOPPprSxgEA2cNzGL355puaNWtW/PWN93sWLlyojRs36vTp09q6dasuXbqkcDisWbNmaefOnQoEAqnrGgCQVTyHUVlZmW63tuqBA/2zICMAIHuk/QEGALfm+19vea75+Ft5SV1r8oJlnmve+LMfe675x1l/47nm33+13HONJEWmJ1WGAYqFUgEA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJhjoVQgg/R80J5UXf5fea/75LluzzUjfcM912z66n/3XCNJDz+63HPNyFfeSOpaSD9mRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMyxUCpg5Pr0r3mu+adv353UtUq+dt5zTTKLnibjJx/+66TqRu55M8WdwBIzIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOZYKBX4DN+kkqTqfvun3hcV3fTHWzzXzLj7quea/hRz1zzXHP+wOLmLXW9Nrg4DEjMjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5Vu1GRhhaXOS55p++W+C5pmbBDs81kvTY711Mqm4ge/6DSZ5rGn881XPNv9hyzHMNsg8zIwCAOcIIAGDOUxjV1tZq8uTJCgQCysvL0/z583X27NmEc5xzqqmpUUFBgUaMGKGysjKdOXMmpU0DALKLpzBqbGxUVVWVjh8/rvr6enV3d6u8vFxdXV3xc9auXav169errq5OJ06cUCgU0pw5c9TZ2Zny5gEA2cHTAwyvvvpqwuvNmzcrLy9PJ0+e1IwZM+Sc04YNG7Rq1SpVVlZKkrZs2aL8/Hxt375dTz31VJ+fGYvFFIvF4q+j0Wgyfw8AQAb7Uu8ZRSIRSVJubq4kqbm5WW1tbSovL4+f4/f7NXPmTB09evSWP6O2tlbBYDC+FRYWfpmWAAAZKOkwcs6purpa06dPV0lJiSSpra1NkpSfn59wbn5+fvzYzVauXKlIJBLfWlpakm0JAJChkv6c0dKlS/X222/ryJEjfY75fL6E1865Pvtu8Pv98vv9ybYBAMgCSc2Mli1bpr179+rQoUMaM2ZMfH8oFJKkPrOg9vb2PrMlAABu8BRGzjktXbpUu3bt0sGDB1VcXJxwvLi4WKFQSPX19fF9V69eVWNjo0pLS1PTMQAg63j6NV1VVZW2b9+uPXv2KBAIxGdAwWBQI0aMkM/n0/Lly7VmzRqNHTtWY8eO1Zo1azRy5Eg9+eSTafkLAAAyn6cw2rhxoySprKwsYf/mzZu1aNEiSdJzzz2nK1euaMmSJfroo480ZcoUvfbaawoEAilpGACQfXzOOWfdxGdFo1EFg0GV6REN9Q2zbge3MfSr9yZVF5kY9lyz4Iev3vmkmzz9++c81wx0K1q9L0QqScde8r7oae7L/9v7ha73eK9B1up219SgPYpEIsrJybntuaxNBwAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwFzS3/SKgWtoOOS55sO/HeW55pniRs81kvRE4IOk6gaypf883XPNP2z8muea0X//fzzXSFJu57Gk6oD+wswIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOVbv7ydU/meS95j9+mNS1nv/D/Z5rykd0JXWtgeyDniuea2bsXZHUte7/z//ouSb3kveVtK97rgAyAzMjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJgjjAAA5lgotZ+cn+899387/pdp6CR1Xrx0X1J1P24s91zj6/F5rrn/R82ea8Z+8IbnGknqSaoKwA3MjAAA5ggjAIA5wggAYI4wAgCYI4wAAOYIIwCAOcIIAGCOMAIAmCOMAADmCCMAgDnCCABgjjACAJjzOeecdROfFY1GFQwGVaZHNNQ3zLodAECSut01NWiPIpGIcnJybnsuMyMAgDnCCABgzlMY1dbWavLkyQoEAsrLy9P8+fN19uzZhHMWLVokn8+XsE2dOjWlTQMAsounMGpsbFRVVZWOHz+u+vp6dXd3q7y8XF1dXQnnzZ07V62trfFt//79KW0aAJBdPH3T66uvvprwevPmzcrLy9PJkyc1Y8aM+H6/369QKJSaDgEAWe9LvWcUiUQkSbm5uQn7GxoalJeXp3Hjxmnx4sVqb2//3J8Ri8UUjUYTNgDA4JJ0GDnnVF1drenTp6ukpCS+v6KiQtu2bdPBgwe1bt06nThxQrNnz1YsFrvlz6mtrVUwGIxvhYWFybYEAMhQSX/OqKqqSvv27dORI0c0ZsyYzz2vtbVVRUVF2rFjhyorK/scj8ViCUEVjUZVWFjI54wAIMN5+ZyRp/eMbli2bJn27t2rw4cP3zaIJCkcDquoqEhNTU23PO73++X3+5NpAwCQJTyFkXNOy5Yt0yuvvKKGhgYVFxffsaajo0MtLS0Kh8NJNwkAyG6e3jOqqqrSL37xC23fvl2BQEBtbW1qa2vTlStXJEmXL1/Ws88+q2PHjun8+fNqaGjQvHnzNHr0aD366KNp+QsAADKfp5nRxo0bJUllZWUJ+zdv3qxFixZpyJAhOn36tLZu3apLly4pHA5r1qxZ2rlzpwKBQMqaBgBkF8+/prudESNG6MCBA1+qIQDA4MPadAAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDAHGEEADBHGAEAzBFGAABzhBEAwBxhBAAwRxgBAMwRRgAAc0OtG7iZc06S1K1rkjNuBgCQtG5dk/Tpv9dvZ8CFUWdnpyTpiPYbdwIASIXOzk4Fg8HbnuNzXySy+tH169f1/vvvKxAIyOfzJRyLRqMqLCxUS0uLcnJyjDocGBiLXoxDL8bhU4xFr4EwDs45dXZ2qqCgQHfddft3hQbczOiuu+7SmDFjbntOTk7OoL7JPoux6MU49GIcPsVY9LIehzvNiG7gAQYAgDnCCABgLqPCyO/3a/Xq1fL7/datmGMsejEOvRiHTzEWvTJtHAbcAwwAgMEno2ZGAIDsRBgBAMwRRgAAc4QRAMAcYQQAMEcYAQDMEUYAAHOEEQDA3P8DZ6yam7DUFooAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check one of the data set\n",
    "image, label = train_data[0]\n",
    "\n",
    "# Print the image\n",
    "plt.matshow(image[0])\n",
    "\n",
    "# Print the output\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000, 60000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the size of datasets\n",
    "len(train_data.data), len(test_data.data), len(train_data.targets), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: <torch.utils.data.dataloader.DataLoader object at 0x000001E3F4471AC0>, <torch.utils.data.dataloader.DataLoader object at 0x000001E3F44713D0>\n",
      "Length of train dataloader: 600 batches of 100\n",
      "Length of test dataloader: 10000\n"
     ]
    }
   ],
   "source": [
    "# What this step is practically doing is converting all the data into batches of 32\n",
    "# And returning the iterables to us\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Dataloaders: {train_data_loader}, {test_data_loader}\")\n",
    "print(f\"Length of train dataloader: {len(train_data_loader)} batches of {100}\")\n",
    "print(f\"Length of test dataloader: {len(test_data_loader)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_layers, hidden_units, activation_func, output_shape, subtract=False):\n",
    "        # Call the super class's init function\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            if(subtract == True):\n",
    "                self.layer_stack.append(nn.Linear(in_features=self.hidden_units, out_features=self.hidden_units - 50))\n",
    "            else:\n",
    "                self.layer_stack.append(nn.Linear(in_features=self.hidden_units, out_features=self.hidden_units))\n",
    "            \n",
    "            if(subtract == True):\n",
    "                self.hidden_units -= 50\n",
    "                \n",
    "            if activation_func == \"t\":\n",
    "                self.layer_stack.append(nn.Tanh())\n",
    "            elif activation_func == \"s\":\n",
    "                self.layer_stack.append(nn.Sigmoid())\n",
    "            elif activation_func == \"r\":\n",
    "                self.layer_stack.append(nn.ReLU())\n",
    "        \n",
    "        self.layer_stack.append(nn.Linear(in_features=self.hidden_units, out_features=output_shape))\n",
    "        self.layer_stack.append(nn.LogSoftmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTModel(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (4): Sigmoid()\n",
       "    (5): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (6): Sigmoid()\n",
       "    (7): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (8): Sigmoid()\n",
       "    (9): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (10): LogSoftmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistModel = MNISTModel(\n",
    "    input_shape=784,\n",
    "    hidden_layers=3,\n",
    "    hidden_units=100,\n",
    "    output_shape=10,\n",
    "    activation_func=\"s\"\n",
    ")\n",
    "mnistModel.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------Training---------\n",
    "epochs = 10\n",
    "\n",
    "# Create the training and testing loop\n",
    "for epoch in range(epochs):    \n",
    "        \n",
    "    for batch, (X, y) in enumerate(train_data_loader):\n",
    "        # Train the model\n",
    "        mnistModel.train()\n",
    "        # Generate Value\n",
    "        y_pred = mnistModel(X)\n",
    "        # Generate loss from loss function\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Optimize :)\n",
    "        # Apparently this sets all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Back Propagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # if (batch+1) % 2 == 0:\n",
    "        #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # We loop through all the test data\n",
    "    for images, labels in test_data_loader:\n",
    "        # Generate output for one model\n",
    "        outputs = mnistModel(images)\n",
    "        # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted[0])\n",
    "        y_true.append(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 967    0    1    1    1    3    1    1    2    3]\n",
      " [   0 1127    3    1    0    1    1    0    2    0]\n",
      " [   5    6  987   11    1    0    1   13    8    0]\n",
      " [   0    0    1  992    0    3    0    3    6    5]\n",
      " [   0    1    3    0  941    3    4    2    6   22]\n",
      " [   1    0    0   24    1  841    1    0   15    9]\n",
      " [   7    2    3    0   10    7  922    0    7    0]\n",
      " [   1   11    6    3    0    0    0  996    2    9]\n",
      " [   2    2    2    7    1    2    1    5  948    4]\n",
      " [   1    4    0   11    5    2    1    6   11  968]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for ANN :\\n%s\\n\"\n",
    "      % (metrics.classification_report(y_true, y_pred, digits=6)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_through_12():\n",
    "    hidden_sizes = [2, 3]\n",
    "    neurons_in_hidden_layers = [100, 150]\n",
    "    activation_functions = [\"t\", \"s\", \"r\"]\n",
    "    epochs = 100\n",
    "    model_no = 0\n",
    "\n",
    "    for hs in hidden_sizes:\n",
    "        for nihl in neurons_in_hidden_layers:\n",
    "            for af in activation_functions:\n",
    "                model_no += 1\n",
    "                \n",
    "                mnistModel = MNISTModel(\n",
    "                    input_shape=784,\n",
    "                    hidden_layers=hs,\n",
    "                    hidden_units=nihl,\n",
    "                    output_shape=10,\n",
    "                    activation_func=af\n",
    "                )\n",
    "                \n",
    "                print(f\"Model #{model_no}\")\n",
    "                \n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)\n",
    "\n",
    "                # ---------Training---------\n",
    "                # Create the training and testing loop\n",
    "                for epoch in range(epochs):    \n",
    "                        \n",
    "                    for batch, (X, y) in enumerate(train_data_loader):\n",
    "                        # Train the model\n",
    "                        mnistModel.train()\n",
    "                        # Generate Value\n",
    "                        y_pred = mnistModel(X)\n",
    "                        # Generate loss from loss function\n",
    "                        loss = loss_fn(y_pred, y)\n",
    "                        # Optimize :)\n",
    "                        # Apparently this sets all gradients to zero\n",
    "                        optimizer.zero_grad()\n",
    "                        # Back Propagate\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        # Debugging Purposes only, Do not use in the loop\n",
    "                        # if (batch+1) % 100 == 0:\n",
    "                        #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                with torch.no_grad():                \n",
    "                    y_pred = []\n",
    "                    y_true = []\n",
    "\n",
    "                    # We loop through all the test data\n",
    "                    for images, labels in test_data_loader:\n",
    "                        # Generate output for one model\n",
    "                        outputs = mnistModel(images)\n",
    "                        # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        y_pred.append(predicted[0])\n",
    "                        y_true.append(labels[0])\n",
    "\n",
    "                print(metrics.confusion_matrix(y_true, y_pred))\n",
    "                print(f\"Classification report for Hidden Layers: {hs}, Neurons in Each Hidden Layer: {nihl}, Activation Function: {af} :\\n%s\\n\"\n",
    "                    % (metrics.classification_report(y_true, y_pred, digits=6)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1-#12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate_through_12()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTModel(\n",
      "  (layer_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=150, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=150, out_features=100, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (6): Tanh()\n",
      "    (7): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (8): LogSoftmax(dim=None)\n",
      "  )\n",
      ")\n",
      "Classification report for Hidden Layers: 2, Neurons in Each Hidden Layer: 150, Activation Function: t :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.967033  0.987755  0.977284       980\n",
      "           1   0.982456  0.986784  0.984615      1135\n",
      "           2   0.933583  0.967054  0.950024      1032\n",
      "           3   0.947368  0.944554  0.945959      1010\n",
      "           4   0.966632  0.943992  0.955178       982\n",
      "           5   0.967630  0.938341  0.952760       892\n",
      "           6   0.978678  0.958246  0.968354       958\n",
      "           7   0.952565  0.957198  0.954876      1028\n",
      "           8   0.939611  0.942505  0.941056       974\n",
      "           9   0.937685  0.939544  0.938614      1009\n",
      "\n",
      "    accuracy                       0.957200     10000\n",
      "   macro avg   0.957324  0.956597  0.956872     10000\n",
      "weighted avg   0.957355  0.957200  0.957191     10000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnistModel = MNISTModel(\n",
    "    input_shape=784,\n",
    "    hidden_layers=2,\n",
    "    hidden_units=150,\n",
    "    output_shape=10,\n",
    "    activation_func=\"t\",\n",
    "    subtract=True\n",
    ")\n",
    "\n",
    "print(mnistModel)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)\n",
    "\n",
    "# ---------Training---------\n",
    "# Create the training and testing loop\n",
    "for epoch in range(1):\n",
    "    for batch, (X, y) in enumerate(train_data_loader):\n",
    "        # Train the model\n",
    "        mnistModel.train()\n",
    "        # Generate Value\n",
    "        y_pred = mnistModel(X)\n",
    "        # Generate loss from loss function\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Optimize :)\n",
    "        # Apparently this sets all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Back Propagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Debugging Purposes only, Do not use in the loop\n",
    "        # if (batch+1) % 100 == 0:\n",
    "        #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # We loop through all the test data\n",
    "    for images, labels in test_data_loader:\n",
    "        # Generate output for one model\n",
    "        outputs = mnistModel(images)\n",
    "        # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted[0])\n",
    "        y_true.append(labels[0])\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "print(f\"Classification report for Hidden Layers: 2, Neurons in Each Hidden Layer: 150, Activation Function: t :\\n%s\\n\" % (metrics.classification_report(y_true, y_pred, digits=6)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistModel = MNISTModel(\n",
    "    input_shape=784,\n",
    "    hidden_layers=2,\n",
    "    hidden_units=150,\n",
    "    output_shape=10,\n",
    "    activation_func=\"s\"\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)\n",
    "\n",
    "# ---------Training---------\n",
    "# Create the training and testing loop\n",
    "for epoch in range(100):\n",
    "    for batch, (X, y) in enumerate(train_data_loader):\n",
    "        # Train the model\n",
    "        mnistModel.train()\n",
    "        # Generate Value\n",
    "        y_pred = mnistModel(X)\n",
    "        # Generate loss from loss function\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Optimize :)\n",
    "        # Apparently this sets all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Back Propagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Debugging Purposes only, Do not use in the loop\n",
    "        # if (batch+1) % 100 == 0:\n",
    "        #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # We loop through all the test data\n",
    "    for images, labels in test_data_loader:\n",
    "        # Generate output for one model\n",
    "        outputs = mnistModel(images)\n",
    "        # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted[0])\n",
    "        y_true.append(labels[0])\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "print(f\"Classification report for Hidden Layers: 2, Neurons in Each Hidden Layer: 150, Activation Function: t :\\n%s\\n\" % (metrics.classification_report(y_true, y_pred, digits=6)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistModel = MNISTModel(\n",
    "    input_shape=784,\n",
    "    hidden_layers=2,\n",
    "    hidden_units=150,\n",
    "    output_shape=10,\n",
    "    activation_func=\"r\"\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mnistModel.parameters(), lr=0.001)\n",
    "\n",
    "# ---------Training---------\n",
    "# Create the training and testing loop\n",
    "for epoch in range(100):\n",
    "    for batch, (X, y) in enumerate(train_data_loader):\n",
    "        # Train the model\n",
    "        mnistModel.train()\n",
    "        # Generate Value\n",
    "        y_pred = mnistModel(X)\n",
    "        # Generate loss from loss function\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Optimize :)\n",
    "        # Apparently this sets all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Back Propagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Debugging Purposes only, Do not use in the loop\n",
    "        # if (batch+1) % 100 == 0:\n",
    "        #         print (f'Epoch [{epoch+1}/{epochs}], Step[{batch+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # We loop through all the test data\n",
    "    for images, labels in test_data_loader:\n",
    "        # Generate output for one model\n",
    "        outputs = mnistModel(images)\n",
    "        # Max returns (value ,index) i.e we need to check which digit has higher probability\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted[0])\n",
    "        y_true.append(labels[0])\n",
    "\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "print(f\"Classification report for Hidden Layers: 2, Neurons in Each Hidden Layer: 150, Activation Function: t :\\n%s\\n\" % (metrics.classification_report(y_true, y_pred, digits=6)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
